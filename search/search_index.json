{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hands On GCP - Closer Academy","text":""},{"location":"#introduction","title":"Introduction","text":"<p>This repository contains the code for the Hands On GCP workshop.</p> <p>In this workshop we will build a simple MLOps system in Google Cloud Platform. By end end of it, we should be able to understand how to create resources in Google Cloud, how to use them and how to integrate them with eachother.</p>"},{"location":"#requirements-and-setup","title":"Requirements and Setup","text":"<ul> <li>WSL (Windows Linux Subsystem)<ul> <li>Installation steps: https://learn.microsoft.com/en-us/windows/wsl/install</li> </ul> </li> </ul> <ul> <li>Docker<ul> <li>Download for windows: https://www.docker.com/products/docker-desktop/</li> </ul> </li> </ul> <ul> <li>Visual Studio Code<ul> <li>Download for windows: https://code.visualstudio.com/download</li> </ul> </li> </ul> <p>We are going to use Visual Studio Code with devcontainers. You can learn more about Devcontainers here.</p>"},{"location":"#exercises","title":"Exercises","text":"<ul> <li>Simple MLOps Pipelines</li> </ul>"},{"location":"client_libs_py/","title":"Client Libraries","text":"<p>A client implementation, such as a Python Client or Javascript Client, is a software library designed to facilitate communication and interaction between an application and a specific service, like an API. It allows developers to easily access and utilize the service's functionalities, by abstracting low-level details and providing a more user-friendly interface in the language of choice.</p> <p>The client implementation acts as an API layer between the application and the server, enabling seamless data exchange and requests management. This layer simplifies the process of making API calls, handling authentication, managing connection details, and processing responses from the server.</p> <p>For example, the Google Cloud Platform (GCP) offers BigQuery Python Client and Python Cloud Storage as part of their Cloud Client Libraries. These libraries provide high-level API abstractions that significantly reduce the amount of boilerplate code developers need to write when interacting with BigQuery and Cloud Storage services.</p> <p>Using the GCP BigQuery Python Client, developers can easily query, manage, and load data into BigQuery tables, while the Python Cloud Storage library simplifies file management, uploads, and downloads in Google Cloud Storage. Both libraries embrace the idiomatic style of the Python language, ensuring better integration with the standard library and developers' existing codebases.</p> <p>You can check all the available Client Libraries for python here.</p>"},{"location":"client_libs_py/#bigquery-client-python","title":"Bigquery Client (Python)","text":"<p>You can use the BigQuery Python Client to execute a query and fetch the results:</p> <pre><code># NOTE: pip install google-cloud-bigquery\n\nfrom google.cloud import bigquery\n\n# Initialize the BigQuery client\nclient = bigquery.Client()\n\n# Define your query\nquery = \"\"\"\n    SELECT name, SUM(number) as total\n    FROM `bigquery-public-data.usa_names.usa_1910_current`\n    WHERE year &gt;= 2000\n    GROUP BY name\n    ORDER BY total DESC\n    LIMIT 10\n\"\"\"\n\n# Execute the query\nquery_job = client.query(query)\n\n# Fetch and print the results\nfor row in query_job.result():\n    print(f\"{row.name}: {row.total}\")\n</code></pre>"},{"location":"client_libs_py/#cloud-storage-client-python","title":"Cloud Storage Client (Python)","text":"<p>You can use the Python Cloud Storage Client to upload a file to a GCS bucket and download it back:</p> <pre><code># NOTE: pip install google-cloud-storage\n\nfrom google.cloud import storage\n\n# Initialize the GCS client\nclient = storage.Client()\n\n# Specify your bucket name\nbucket_name = \"your-bucket-name\"\n\n# Get a reference to the bucket\nbucket = client.get_bucket(bucket_name)\n\n# Upload a file\nsource_file_name = \"path/to/your/local/file.txt\"\ndestination_blob_name = \"uploaded_file.txt\"\nblob = bucket.blob(destination_blob_name)\nblob.upload_from_filename(source_file_name)\nprint(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n\n# Download the file\ndownloaded_file_name = \"path/to/your/local/downloaded_file.txt\"\nblob = bucket.blob(destination_blob_name)\nblob.download_to_filename(downloaded_file_name)\nprint(f\"File {destination_blob_name} downloaded to {downloaded_file_name}.\")\n</code></pre>"},{"location":"client_libs_py/#pubsub-client-python","title":"Pub/Sub Client (Python)","text":"<p>You can use the Pub/Sub Python Client to publish a message to the existing topic:</p> <pre><code># NOTE: pip install google-cloud-pubsub\n\nfrom google.cloud import pubsub_v1\n\n# Initialize the Pub/Sub client\npublisher = pubsub_v1.PublisherClient()\n\n# Set your project_id and topic_name\nproject_id = \"your-project-id\"\ntopic_name = \"your-existing-topic-name\"\n\n# Get the topic path\ntopic_path = publisher.topic_path(project_id, topic_name)\n\n# Publish a message\nmessage = \"Hello, World!\"\nmessage_data = message.encode(\"utf-8\")\nfuture = publisher.publish(topic_path, message_data)\nmessage_id = future.result()\nprint(f\"Message published with ID: {message_id}\")\n</code></pre> <p>As Pub/Sub promotes decoupled and flexible architectures, message_data is transformed into a base64-encoded string to ensure language-agnostic compatibility. Therefore, subscribers must decode the base64 message. In Python, this can be done as follows:</p> <pre><code>import base64\n\ndef hello_pubsub(data, context):\n\"\"\"Triggered from a message on a Cloud Pub/Sub topic.\n    Args:\n         event (dict): Event payload.\n         context (google.cloud.functions.Context): Metadata for the event.\n    \"\"\"\n    print(\"This Function was triggered by messageId {} published at {}\".format(context.event_id, context.timestamp))\n\n    if 'data' in event:\n        decoded_msg = base64.b64decode(data['data']).decode('utf-8')\n        # Message is now decoded\n    ## Your Cloud Function Implementation\n</code></pre>"},{"location":"client_libs_py/#setup-python-venv","title":"Setup python venv","text":"<pre><code>python -m venv venv\n</code></pre> <p>Install the Python Extension:</p> <p></p> <p>With VSCode, do <code>CTRL+SHIFT+P</code> and write <code>Select Interpreter</code></p> <p></p> <p>And find the <code>venv</code> python executable.</p> <p></p>"},{"location":"create_resources/","title":"Create Resources (Tables/Buckets/Topics) within GCP","text":""},{"location":"create_resources/#buckets-ui","title":"Buckets (UI)","text":"<ol> <li> <p>Search for the Cloud Storage in the Search bar.</p> <p></p> </li> <li> <p>In the Cloud Storage UI, you'll notice there are no buckets created yet. To create one, click the <code>CREATE</code> button.</p> <p></p> </li> <li> <p>Configurate your bucket</p> <p></p> <ol> <li>Name your bucket and click Continue.</li> <li>Change the storage class from Multi-region to Region. Set the location to europe-west3, as shown in the image, and click Continue.</li> <li>Keep the remaining settings as they are.</li> <li>Click create.</li> </ol> <p>Your configuration should look like this:</p> <p></p> <p>If this popup appears, leave the settings as they are.</p> <p></p> </li> </ol> <p>And now you have your bucket!</p> <p></p> <p>Alternatively, you can create a bucket using Python, other Client Libraries, or even advanced Infrastructure-as-Code tools like Terraform or Pulumi.</p>"},{"location":"create_resources/#what-is-google-cloud-storage-gcs","title":"What is Google Cloud Storage (GCS)?","text":"<p>Google Cloud Storage (GCS) is a scalable, fully-managed, and highly available object storage service provided by Google Cloud Platform. It allows users to store, access, and manage data across multiple storage classes, catering to various use cases like backup, archival, and content delivery. GCS ensures data durability and offers seamless integration with other Google Cloud services.</p>"},{"location":"create_resources/#bigquery-data-sets-and-tables-ui","title":"Bigquery Data Sets and Tables (UI)","text":"<p>Tables are always associated with a <code>data set</code>. First, let's create a <code>data set</code>.</p> <ol> <li>Go to BigQuery:</li> </ol> <p></p> <ol> <li>Click the bullet points icon next to the project name:</li> </ol> <p></p> <ol> <li>Name your data set, change the region, and click <code>CREATE DATA SET</code>:</li> </ol> <p></p> <pre><code>Congratulations! You have a `data set`!\n\nNow, let's create a table:\n</code></pre> <ol> <li>Click the bullets icon next to your data set, and click Create Table:</li> </ol> <p></p> <ol> <li>Configure your table settings:</li> </ol> <p></p> <pre><code>Alternatively, you can define the schema using `JSON`:\n\n![img-bq-6](https://i.imgur.com/UcDK3uC.png)\n</code></pre> <p>And now you have a table too!</p> <p>Remember the location of the Table ID; you might need it later:</p> <p></p> <p>Learn more about tables in the documentation.</p> <p>You can also create Tables with Infrastructure-As-Code tools. Here are the examples for Terraform and for the several Clients for Pulumi</p>"},{"location":"create_resources/#what-is-bigquery","title":"What is Bigquery?","text":"<p>BigQuery is a fully-managed, serverless, petabyte-scale data warehouse by Google Cloud Platform. It enables super-fast SQL queries using the processing power of Google's infrastructure, allowing users to analyze large datasets in real-time. BigQuery is designed for scalability, ease of use, and integration with other Google Cloud services.</p>"},{"location":"create_resources/#json-schema","title":"JSON Schema","text":"<p>Why should you use JSON schemas when possible?</p> <ol> <li> <p>Structure and consistency: JSON schemas define the structure of your data, ensuring consistency across all records in the table. This helps maintain data integrity and makes it easier to query and analyze the data.</p> </li> <li> <p>Validation: By specifying a schema, you can enforce data validation rules, such as data types and required fields, ensuring that only valid data is inserted into the table. This can prevent issues caused by incorrect or incomplete data.</p> </li> <li> <p>Readability: JSON schemas provide a clear and human-readable description of the table's structure, making it easier for team members to understand the data and its organization</p> </li> <li> <p>Interoperability: JSON schemas are a standardized format, which makes it simpler to share and exchange table structures across teams and different systems. This is particularly useful when integrating with other tools or platforms that support JSON schema.</p> </li> <li> <p>Easier data import: When importing data from files (e.g., CSV or JSON) into BigQuery, providing a JSON schema allows BigQuery to map the file's data correctly to the table's columns, preventing import errors and ensuring data consistency.</p> </li> </ol> <p>Here's an example of a JSON schema:</p> <pre><code>[\n{\n\"name\": \"my_required_text_field\",\n\"type\": \"STRING\",\n\"mode\": \"REQUIRED\",\n\"description\": \"A text field\"\n},\n{\n\"name\": \"my_required_integer_field\",\n\"type\": \"INTEGER\",\n\"mode\": \"NULLABLE\",\n\"description\": \"An integer field\"\n},\n{\n\"name\": \"my_nullable_boolean_field\",\n\"type\": \"BOOLEAN\",\n\"mode\": \"NULLABLE\",\n\"description\": \"A boolean field\"\n}\n]\n</code></pre> <p>You can also find it in the folder <code>./infrastructure/bigquery/example_schema.json</code>.</p> <p>You can fiend more about how to define JSON schemas in the Google Documentation.</p>"},{"location":"create_resources/#pubsub-topics-ui","title":"Pub/Sub Topics (UI)","text":"<ol> <li>Search for Topics in the search bar.</li> <li> <p>Click in CREATE TOPIC.</p> <p></p> </li> <li> <p>Define your Topic ID and click CREATE</p> <p></p> <p>In this case, our Topic ID is <code>ingestion_complete</code>.</p> <p>Remember where to find your Topic IDs, it will be useful when instrumenting the python scripts.</p> </li> <li> <p>We have a new topic!</p> </li> </ol> <p></p> <p>It automatically creates a subscription, but lets ignore that for now.</p> <ol> <li> <p>If you go back to the Topics page, it should look like this</p> <p></p> </li> </ol> <p>Like any other resource, we can also create Topics and Subscriptions with IaC.</p> <p>Follow these links for examples for Terraform and Pulumi.</p>"},{"location":"create_resources/#what-is-the-publisher-subscriber-pattern","title":"What is the Publisher-Subscriber pattern?","text":"<pre><code>  The publisher-subscriber (pub-sub) messaging pattern is a communication paradigm where messages are sent by publishers to multiple subscribers, without requiring direct connections between them. Publishers broadcast messages to topics, and subscribers listen to topics they are interested in. This pattern provides a decoupled architecture, allowing for scalability, flexibility, and fault tolerance. Subscribers receive messages asynchronously, enabling them to process events independently, without blocking or waiting for other subscribers. The pub-sub pattern is widely used in distributed systems, event-driven architectures, and messaging applications.\n</code></pre>"},{"location":"create_resources/#what-is-google-pubsub","title":"What is Google Pub/Sub?","text":"<pre><code>  Google Pub/Sub is a real-time messaging service based on the publisher-subscriber pattern, designed for Google Cloud Platform. It enables reliable, scalable, and asynchronous event-driven communication between microservices, applications, and data streams, promoting decoupled and flexible architectures.\n</code></pre>"},{"location":"create_resources/#deploy-cloud-functions-gcloud","title":"Deploy Cloud Functions (gcloud)","text":"<p>In the exercises we will deploy Cloud Functions from a Zip file. For this, we need a bucket specifically for storing the zipped code of the functions. Find more information on GCP documentation, and follow the example in how to create a bucket to create a bucket for the zipped files.</p> <ol> <li> <p>Activate the Cloud Shell</p> <p></p> <p>After a while, you should have a command line in the bottom of your browser. Confirm that you have an active project (green rectangle). If not, contact us.</p> <p></p> </li> <li> <p>Execute the following command:</p> <pre><code>gcloud functions deploy [YOUR_FUNCTION_NAME] \\\n--region=europe-west3 \\\n--runtime=python39 \\\n--source=gs://[ZIPPED_FUNCTIONS_BUCKET]/[ZIP_NAME] \\\n--entry-point=main \\\nTRIGGER_FLAGS\n</code></pre> <p><code>--region</code>: Deployment Region. - List of Regions</p> <p><code>--runtime</code>: The execution environment. - Available environments (We will use python39)</p> <p><code>--source</code>: Source code of the function. - There are several ways to access the source code. We will use Deploy from Cloud Storage. But you can deploy from a source repository or directly from a local machine (your PC).</p> <p><code>--entry-point</code>: The function/code executed when the Cloud Function runs. - Learn more here for event-driven functions and for http functions.</p> <p><code>TRIGGER_FLAGS</code>: The trigger type of the Cloud Function. - See more in the table here. - In our case, we will use three trigger types for the three cloud functions. <code>--trigger-bucket</code>, <code>--trigger-topic</code> and <code>--trigger-http</code>.</p> <p>If this shows up:</p> <p></p> <p>Type <code>y</code></p> </li> <li> <p>Check the status of your deployment.</p> <p>Search for Cloud Build</p> <p></p> <p>And check if you are in Region europe-west3. Something similar to this should show up:</p> <p></p> <p>If the cloud function was deployed with success, you'll get a green check</p> <p></p> <p>Go to the Cloud Functions UI, and check that your function was deployed with success.</p> <p></p> <p>You can click the function name and check it's properties, configurations and even source code deployed.</p> <p>For example, we can check if the trigger of a cloud function in the <code>Trigger</code> page.</p> <p></p> <p>If your deployment didn't work, let us know and we'll help you.</p> </li> </ol>"},{"location":"exercises/simple_mlops/","title":"Simple MLOps Pipelines","text":""},{"location":"exercises/simple_mlops/#architecture","title":"Architecture","text":"<p>Please, refer to this document for a detailed explanation of the architecture of the first project.</p>"},{"location":"exercises/simple_mlops/#google-cloud-services","title":"Google Cloud Services","text":"<p>Please, refer to this document for an introduction to some of the Google Cloud services used in this workshop.</p>"},{"location":"exercises/simple_mlops/architecture/","title":"Architecure","text":"<p>We are going to build a simple MLOps project in Google Cloud Platform using <code>Cloud Storage</code>, <code>Cloud Functions</code>, <code>Bigquery</code> and <code>Pubsub</code>.</p> <p>Our minimal MLOps system should look like this in the end:</p> <p></p>"},{"location":"exercises/simple_mlops/architecture/#1-ingestion","title":"1. Ingestion","text":"<ol> <li>Cloud Function <code>Ingest Data</code> monitors the <code>yourname-lz</code> for new files.</li> <li>Upon detecting a new file, <code>Ingest Data</code> writes its contents to the BigQuery table <code>Titanic Raw</code>.</li> <li>Once sucessufully complete, a message is sent to the <code>yourname-ingestion-complete</code> topic, notifying subscribers about the new data in BigQuery.</li> </ol>"},{"location":"exercises/simple_mlops/architecture/#2-staging-to-facts","title":"2. Staging to Facts","text":"<ol> <li>The function <code>Query to Facts</code> is activated, and executes a query which moves new data from the <code>Titanic Raw</code> table to <code>Titanic Facts</code>.</li> <li>Once sucessfully complete, a message is sent to the topic <code>yourname-update-facts-complete</code>, notifying its subscribers that the move from raw to facts is complete.</li> </ol>"},{"location":"exercises/simple_mlops/architecture/#3-train-model","title":"3. Train model","text":"<ol> <li>The <code>train_model</code> Cloud Function, is activated by the topic<code>yourname-update-facts-complete</code>, and pulls the data from the bigquery table <code>Titanic Facts</code>.</li> <li>Once the training of the model is done, the model file is uplodaded to the <code>yourname-models</code> bucket.</li> </ol>"},{"location":"exercises/simple_mlops/architecture/#4-predictions","title":"4. Predictions","text":"<ol> <li>A model is retrieved from the  <code>yourname-models</code> bucket and lodaded into the Cloud Function. The function is ready to take requests.</li> <li>A request for a prediction is made by an user.</li> <li>The request is saved to the <code>Titanic Predictions</code> table</li> <li>A response is given back to the user.</li> </ol>"},{"location":"exercises/simple_mlops/dataset/","title":"Meet the dataset","text":"<p>We will use the Titanic Dataset available pretty much anywhere.</p> <p>The columns and their types are the following</p> Column name Python data type Bigquery data type Description PassengerId int INT64 Unique identifier for each passenger Survived bool BOOLEAN Survival status (False = No, True = Yes) Pclass int INT64 Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd) Name str STRING Full name of the passenger Sex str STRING Gender (male or female) Age float FLOAT64 Age in years SibSp int INT64 Number of siblings/spouses aboard the Titanic Parch int INT64 Number of parents/children aboard the Titanic Ticket str STRING Ticket number Fare float FLOAT64 Passenger fare Cabin str STRING Cabin number Embarked str STRING Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton) OPTIONAL: set_type str STRING Set type (Train / Test / Validation) <p>So, when creating the Tables, you have to create the schema accodingly. </p> <p>The dataset is available at <code>./dataset/titanic.csv</code>.</p>"},{"location":"exercises/simple_mlops/step1/","title":"Load a file from Cloud Storage to a Bigquery Table using a Cloud Function","text":"<ul> <li>Load a file from Cloud Storage to a Bigquery Table using a Cloud Function</li> <li>Introduction</li> <li>Tasks</li> <li>Create the Google Cloud Resources<ul> <li>1. Create a BigQuery Dataset</li> <li>2. Create a BigQuery Table</li> <li>3. Create a Google Cloud Storage Bucket</li> <li>4. Create the pubsub topic for ingestion complete</li> </ul> </li> <li>Update the Cloud Function Code</li> <li>Deploy the cloud function</li> <li>Hints<ul> <li>Cloud Events</li> </ul> </li> <li>Documentation</li> </ul>"},{"location":"exercises/simple_mlops/step1/#introduction","title":"Introduction","text":"<p>In this exercise, we will create the <code>Ingest Data</code> Cloud Function, that will perform the following tasks:</p> <ol> <li> <p>The <code>Ingest Data</code> function will actively monitor the <code>[YOURNAME]-lz</code> Google Cloud Storage bucket for new files. This is achieved by configuring a trigger topic (PubSub) in the Cloud Function to listen for object creation events in the specified bucket.</p> </li> <li> <p>When a new file is detected, the <code>Ingest Data</code> function will read the contents of the file and write the data into a BigQuery table named <code>Titanic Raw</code>. The function will leverage the BigQuery Python client library to facilitate this process, efficiently importing the data from the file into the specified table.</p> </li> <li> <p>After successfully importing the data into BigQuery, the <code>Ingest Data</code> function will send a message to the <code>yourname-ingestion-complete</code> topic in Google Cloud Pub/Sub. This message will notify all subscribers that new data has been loaded into BigQuery, allowing them to react accordingly, such as by initiating further data processing tasks.</p> </li> </ol> <p>The Cloud Function <code>Ingest Data</code> will utilize the Google Cloud Storage, BigQuery, and Pub/Sub client libraries for these tasks. Our goal in this exercise is to fix the code for this function to make it function preperly and deploy it to Google Cloud.</p> <p>The resources needed these tasks are:</p> <ul> <li>One Bigquery Data Set and one bigquery Table</li> <li>The table schema is at: <code>./infrastructure/bigquery/titanic_schema_raw.json</code></li> <li>One GCS Bucket named <code>[prefix]-landing-zone-bucket</code> where you will drop the files once the function is ready</li> <li>One GCS Bucket named <code>[prefix]-functions-bucket</code> where you will deploy the function source code from.</li> <li>One Topic named <code>[prefix]-ingestion-complete</code>, to where the function will send a message once complete.</li> </ul> <p>The outline of the Cloud Function code is available at <code>functions/simple_mlops/a_ingest_data/app/main.py</code>.</p> <pre><code>.\n\u2514\u2500\u2500 a_ingest_data/\n    \u251c\u2500\u2500 app/\n    \u2502   \u251c\u2500\u2500 funcs/\n    \u2502   \u2502   \u251c\u2500\u2500 models.py # Models to make typechecking easier.\n    \u2502   \u2502   \u251c\u2500\u2500 gcp_apis.py # Functions to call google services.\n    \u2502   \u2502   \u2514\u2500\u2500 transform.py # Transformations of data into structures\n    \u2502   \u251c\u2500\u2500 main.py # Main module and entry point for the Cloud Function\n    \u2502   \u2514\u2500\u2500 requirements.txt # Requirements for the function execution.\n    \u251c\u2500\u2500 config/\n    \u2502   \u2514\u2500\u2500 dev.env.yaml # Environment variables that will ship with the function deployment\n    \u2514\u2500\u2500 tests/\n        \u2514\u2500\u2500 test_*.py # Unit tests.\n</code></pre>"},{"location":"exercises/simple_mlops/step1/#tasks","title":"Tasks","text":"<ul> <li> Create the Google Cloud Resources</li> <li> Update the Cloud Function Code</li> <li> Deploy the Cloud Function</li> <li> Test the Cloud Function</li> </ul>"},{"location":"exercises/simple_mlops/step1/#create-the-google-cloud-resources","title":"Create the Google Cloud Resources","text":"<p>Here are the resources necessary to complete the exercise:</p> <p>You can create the resources with Cloud Shell or in the Console. The end result will be the same. When creating a resource, choose either to create it with the cloud shell or the console, but not both.</p> <p>For Cloud Shell, set these variables:</p> <pre><code>export PROJECT_ID=$(gcloud config get-value project)\nexport PROJECT_NAME=$(gcloud config get-value project)\nexport PROJECT_NUMBER=$(gcloud projects describe $PROJECT_ID --format='value(projectNumber)')\nexport REGION=europe-west3\nexport YOURNAME=your_name_in_lowercase\n</code></pre> <p></p>"},{"location":"exercises/simple_mlops/step1/#1-create-a-bigquery-dataset","title":"1. Create a BigQuery Dataset","text":"<p>With Cloud Shell (Copy-paste):</p> <pre><code>bq mk \\\n--project_id ${PROJECT_ID} \\\n--location ${REGION} \\\n--dataset \\\n--description \"Dataset for the Titanic dataset\" \\\n--label=owner:${YOURNAME} \\\n--label=project:${PROJECT_NAME} \\\n--label=purpose:academy \\\n${YOURNAME}_titanic\n</code></pre> <p>Reference: bq mk --dataset</p> <p>With the Console:</p> <ol> <li> <p>Go to BigQuery:</p> <p></p> </li> <li> <p>Click the bullet points icon next to the project name:</p> <p></p> </li> <li> <p>Name your data set, change the region, and click <code>CREATE DATA SET</code>:</p> <p></p> <p>Congratulations! You have a <code>data set</code>!</p> </li> <li> <p>Edit the labels</p> <p>Click in the recently created dataset. </p> <p>And add the labels</p> <p></p> </li> </ol>"},{"location":"exercises/simple_mlops/step1/#2-create-a-bigquery-table","title":"2. Create a BigQuery Table","text":"<p>With Cloud Shell (Copy-paste):</p> <pre><code>bq mk \\\n--project_id ${PROJECT_ID} \\\n--table \\\n--description \"Table for the Titanic dataset\" \\\n--label=owner:${YOURNAME} \\\n--label=project:${PROJECT_NAME} \\\n--label=purpose:academy \\\n--label=dataset:titanic \\\n${YOURNAME}_titanic.titanic_raw \\\n./infrastructure/bigquery/titanic_schema_raw.json\n</code></pre> <p>Reference: bq mk --table</p> <p>With the console:</p> <ol> <li> <p>Click the bullets icon next to your data set, and click Create Table:</p> <p></p> </li> <li> <p>Configure your table:</p> <p></p> <ol> <li>Make sure it's in your dataset created in the step before</li> <li>Name your dataset <code>titanic_raw</code></li> <li>Copy the schema in <code>infrastructure/bigquery/titanic_schema_raw.json</code> and paste it</li> <li>Create the table.</li> </ol> </li> <li> <p>Add the labels.</p> <p></p> <p>To add the labels go to <code>EDIT DETAILS</code>, and the same way as the dataset, add the labels. Include the <code>Dataset</code> : <code>titanic</code> label.</p> </li> </ol>"},{"location":"exercises/simple_mlops/step1/#3-create-a-google-cloud-storage-bucket","title":"3. Create a Google Cloud Storage Bucket","text":"<pre><code>gsutil mb \\\n-c regional \\\n-l ${REGION} \\\n-p ${PROJECT_ID} \\\ngs://${YOURNAME}-lz\n\ngsutil label ch -l owner:${YOURNAME} gs://${YOURNAME}-lz\ngsutil label ch -l project:${PROJECT_NAME} gs://${YOURNAME}-lz\ngsutil label ch -l purpose:academy gs://${YOURNAME}-lz\n</code></pre> <p>Reference: gsutil mb, gsutil label</p> <p>With the console:</p> <ol> <li> <p>Search for the Cloud Storage in the Search bar.</p> <p></p> </li> <li> <p>In the Cloud Storage UI, you'll notice there are no buckets created yet. To create one, click the <code>CREATE</code> button.</p> <p></p> </li> <li> <p>Configurate your bucket</p> <p></p> <ol> <li>Name your bucket and click Continue.</li> <li>Change the storage class from Multi-region to Region. Set the location to europe-west3, as shown in the image, and click Continue.</li> <li>Keep the remaining settings as they are.</li> <li>Click create.</li> </ol> <p>Your configuration should look like this:</p> <p></p> <p>If this popup appears, leave the settings as they are.</p> <p></p> </li> </ol> <p>And now you have your bucket!</p> <p></p> <p>Alternatively, you can create a bucket using Python, other Client Libraries, or even advanced Infrastructure-as-Code tools like Terraform or Pulumi.</p>"},{"location":"exercises/simple_mlops/step1/#4-create-the-pubsub-topic-for-ingestion-complete","title":"4. Create the pubsub topic for ingestion complete","text":"<p>With Cloud Shell:</p> <pre><code>gcloud pubsub topics create ${YOURNAME}-ingestion-complete \\\n--project=${PROJECT_ID} \\\n--labels=owner=${YOURNAME},project=${PROJECT_NAME},purpose=academy\n</code></pre> <p>With the Cloud Console:</p> <ol> <li>Search for Topics in the search bar.</li> <li> <p>Click in CREATE TOPIC.</p> <p></p> </li> <li> <p>Define your Topic ID and click CREATE</p> <p>The topic ID should be <code>[your_name]-ingestion-complete</code></p> <p></p> <p>In this case, our Topic ID is <code>ingestion_complete</code>.</p> <p>Remember where to find your Topic IDs, it will be useful when instrumenting the python scripts.</p> </li> <li> <p>Verify your topic was created</p> </li> </ol> <p></p> <p>It automatically creates a subscription, but lets ignore that for now.</p> <p>Now we are ready to move to the cloud function code.</p>"},{"location":"exercises/simple_mlops/step1/#update-the-cloud-function-code","title":"Update the Cloud Function Code","text":"<p>Here are the steps necessary to complete the exercise:</p> <ol> <li> <p>Create the client objects: Use the Google Cloud Storage API, BigQuery API, and PubSub API to create respective client objects.</p> <pre><code>################\n# 1. Clients ###\n################\nstorage_client = 'Create a storage client here, with the correct project ID argument'\nbigquery_client = 'Create a bigquery client here, with the correct project ID argument'\npublisher = 'Create a publisher client here, with the correct project ID argument'\n\nreturn models.GCPClients(\n    storage_client=storage_client,\n    bigquery_client=bigquery_client,\n    publisher=publisher\n)\n</code></pre> </li> <li> <p>Set Environment Variables</p> <p>In the <code>a_ingest_data/config/dev.env.yaml</code> file, change the environment variables for the correct ones.</p> <pre><code>##############################\n# 2. Environment variables ###\n##############################\n</code></pre> <pre><code>_GCP_PROJECT_ID: \"The GCP project ID where the resources are located\"\n_BIGQUERY_DATASET_ID: \"The BigQuery dataset ID you created\"\n_BIGQUERY_TABLE_ID: \"The BigQuery table ID where you will store the data\"\n_TOPIC_INGESTION_COMPLETE: \"The Pub/Sub topic ID where you will send a message once the data is ingested\"\n</code></pre> </li> <li> <p>Send the correct arguments to the <code>storage_download_blob_as_string</code> function</p> <pre><code>#########################################################\n# 3. Correct the arguments below to download the file ###\n#########################################################\nfile_contents = gcp_apis.storage_download_blob_as_string(\n    CS='??',\n    bucket_name='??',\n    file_path='??',\n)\n</code></pre> </li> <li> <p>Insert Rows into BigQuery: Corrent the arguments in the <code>bigquery_insert_json_row</code> function to insert data into the BigQuery table.</p> <pre><code>###############################################################\n# 4. Correct the arguments below to insert data into bigquery #\n###############################################################\nerrors = [\n    gcp_apis.bigquery_insert_json_row(\n        BQ='??',\n        table_fqn='??',\n        row=[datapoint.to_dict()]\n    ) for datapoint in transform.titanic_transform(datapoints=datapoints)]\n\nif any(errors):\n    raise ValueError(f\"Errors found: {errors}\")\n</code></pre> </li> <li> <p>Publish Message: Correct the arguments in the <code>pubsub_publish_message</code> function, to publish a message.</p> <pre><code>#########################################################\n# 5. Correct the arguments below to publish a message ###\n#########################################################\ngcp_apis.pubsub_publish_message(\n    PS='??',\n    project_id='??',\n    topic_id='??',\n    message=f\"I finished ingesting the file {[change me]}!!\",\n    attributes={},\n)\n</code></pre> </li> </ol>"},{"location":"exercises/simple_mlops/step1/#deploy-the-cloud-function","title":"Deploy the cloud function","text":"<p>You can check the deployment here in Cloud Build</p> <pre><code>FUNCTION_NAME=\"ingest_data\"\nYOURNAME=\"your_name_in_lowercase\"\n\ngcloud beta functions deploy $YOURNAME-$FUNCTION_NAME \\\n--gen2 --cpu=1 --memory=512MB \\\n--region=europe-west3 \\\n--runtime=python311 \\\n--source=functions/simple_mlops/a_ingest_data/app/ \\\n--env-vars-file=functions/simple_mlops/a_ingest_data/config/dev.env.yaml \\\n--entry-point=main \\\n--trigger-event-filters=\"type=google.cloud.storage.object.v1.finalized\" \\\n--trigger-event-filters=\"bucket=$YOURNAME-lz\"\n</code></pre> <p>Reference: gcloud functions deploy</p>"},{"location":"exercises/simple_mlops/step1/#hints","title":"Hints","text":""},{"location":"exercises/simple_mlops/step1/#cloud-events","title":"Cloud Events","text":"<p>The CloudEvent is an object with the following structure:</p> <pre><code>{\n\"attributes\": {\n\"specversion\": \"1.0\",\n\"id\": \"1234567890\",\n\"source\": \" //pubsub.googleapis.com/projects/[The GCP Project of the topic]/topics/[The topic name]\",\n\"type\": \"google.cloud.pubsub.topic.v1.messagePublished\",\n\"datacontenttype\": \"application/json\",\n\"time\": \"2020-08-08T00:11:44.895529672Z\"\n},\n\"data\": {\n\"message\": {\n\"_comment\": \"data is base64 encoded string of 'Hello World'\",\n\"data\": \"SGVsbG8gV29ybGQ=\"\n}\n}\n}\n</code></pre> <p>You can read the CloudEvent specification in the github page.</p> <p>When a Cloud Storage event is passed to a CloudEvent function, the data payload is of type StorageObjectData. This protobuf translates to the following <code>JSON</code>:</p> <pre><code>{\n\"attributes\": {\n\"specversion\": \"1.0\",\n\"id\": \"1234567890\",\n\"source\": \"//storage.googleapis.com/projects/_/buckets/[Bucket Name]\",\n\"type\": \"google.cloud.storage.object.v1.finalized\",\n\"datacontenttype\": \"application/json\",\n\"time\": \"2020-08-08T00:11:44.895529672Z\"\n},\n\"data\": {\n\"name\": \"folder/myfile.csv [File path inside the bucket]\",\n\"bucket\": \"[Bucket Name]\",\n\"contentType\": \"application/json\",\n\"metageneration\": \"1\",\n\"timeCreated\": \"2020-04-23T07:38:57.230Z\",\n\"updated\": \"2020-04-23T07:38:57.230Z\"\n}\n}\n</code></pre> <p>Read more on how to deploy a function that listens to a Cloud Storage bucket event at:</p> <ul> <li>Codelabs - Triggering Event Processing from Cloud Storage using Eventarc and Cloud Functions (2nd gen)</li> <li>Cloud Storage Tutorial (2nd gen)</li> </ul>"},{"location":"exercises/simple_mlops/step1/#documentation","title":"Documentation","text":"<p>Does not work without the code correct.</p>"},{"location":"exercises/simple_mlops/step2/","title":"Query Staging to Facts","text":"<ul> <li>Query Staging to Facts</li> <li>Introduction</li> <li>Tasks</li> <li>Create the Google Cloud Resources<ul> <li>1. Create a BigQuery Table</li> <li>2. Create the pubsub topic for update facts complete</li> </ul> </li> <li>Update the Cloud Function Code</li> <li>Deploy the cloud function</li> <li>Documentation</li> </ul>"},{"location":"exercises/simple_mlops/step2/#introduction","title":"Introduction","text":"<p>In this exercise, we will create the <code>Query To Facts</code> Cloud Function, that will perform the following tasks:</p> <ol> <li> <p>Activated by the topic <code>[yourname]-ingestion-complete</code>.</p> </li> <li> <p>It will send a query to be executed in BigQuery. This query is already done, and will move the data from the staging table to a facts table.</p> </li> <li> <p>After successfully executing the query, this function will send a message to the topic <code>[yourname]-update-facts-complete</code>.</p> </li> </ol> <p>The Cloud Function <code>Ingest Data</code> will utilize the BigQuery, and Pub/Sub client libraries for these tasks. Our goal in this exercise is to fix the code for this function to make it function preperly and deploy it to Google Cloud.</p> <p>The resources needed these tasks are:</p> <ul> <li>The already created Data Set in step 1.</li> <li>One Bigquery table, <code>Titanic Facts</code></li> <li>The table schema is at: <code>./infrastructure/bigquery/facts_titanic_schema.json</code></li> <li>Two Pub/Sub topics, the one already created, and one named <code>[yourname]-update-facts-complete</code>, to where the function will send a message once complete.</li> </ul> <p>The outline of the Cloud Function code is available at <code>functions/simple_mlops/2_update_facts/app</code>.</p> <pre><code>.\n\u2514\u2500\u2500 b_update_facts/\n    \u251c\u2500\u2500 app/\n    \u2502   \u251c\u2500\u2500 funcs/\n    \u2502   \u2502   \u251c\u2500\u2500 models.py # Models to make typechecking easier.\n    \u2502   \u2502   \u251c\u2500\u2500 gcp_apis.py # Functions to call google services.\n    \u2502   \u2502   \u2514\u2500\u2500 common.py # Common functions (Utils).\n    \u2502   \u251c\u2500\u2500 main.py # Main module and entry point for the Cloud Function\n    \u2502   \u2514\u2500\u2500 requirements.txt # Requirements for the function execution.\n    \u251c\u2500\u2500 config/\n    \u2502   \u2514\u2500\u2500 dev.env.yaml # Environment variables that will ship with the function deployment\n    \u2514\u2500\u2500 tests/\n        \u2514\u2500\u2500 test_*.py # Unit tests.\n</code></pre>"},{"location":"exercises/simple_mlops/step2/#tasks","title":"Tasks","text":"<ul> <li> Create the Google Cloud Resources</li> <li> Update the Cloud Function Code</li> <li> Test the Cloud Function</li> <li> Deploy the Cloud Function</li> </ul>"},{"location":"exercises/simple_mlops/step2/#create-the-google-cloud-resources","title":"Create the Google Cloud Resources","text":"<p>Here are the resources necessary to complete the exercise:</p> <p>You can create the resources with Cloud Shell or in the Console. The end result will be the same. When creating a resource, choose either to create it with the cloud shell or the console, but not both.</p> <p>For Cloud Shell, set these variables:</p> <pre><code>export PROJECT_ID=$(gcloud config get-value project)\nexport PROJECT_NAME=$(gcloud config get-value project)\nexport PROJECT_NUMBER=$(gcloud projects describe $PROJECT_ID --format='value(projectNumber)')\nexport REGION=europe-west3\nexport YOURNAME=your_name_in_lowercase\n</code></pre> <p></p>"},{"location":"exercises/simple_mlops/step2/#1-create-a-bigquery-table","title":"1. Create a BigQuery Table","text":"<p>With Cloud Shell (Copy-paste):</p> <pre><code>bq mk \\\n--project_id ${PROJECT_ID} \\\n--table \\\n--description \"Facts table for the Titanic dataset\" \\\n--label=owner:${YOURNAME} \\\n--label=project:${PROJECT_NAME} \\\n--label=purpose:academy \\\n--label=dataset:titanic \\\n${YOURNAME}_titanic.titanic_facts \\\n./infrastructure/bigquery/facts_titanic_schema.json\n</code></pre> <p>Reference: bq mk --table</p> <p>With the console:</p> <p>Same as step 1, but now with the schema <code>facts_titanic_schema.json</code></p>"},{"location":"exercises/simple_mlops/step2/#2-create-the-pubsub-topic-for-update-facts-complete","title":"2. Create the pubsub topic for update facts complete","text":"<p>With Cloud Shell:</p> <pre><code>gcloud pubsub topics create ${YOURNAME}-update-facts-complete \\\n--project=${PROJECT_ID} \\\n--labels=owner=${YOURNAME},project=${PROJECT_NAME},purpose=academy\n</code></pre> <p>With the Cloud Console:</p> <p>Same as before, but now with the name <code>[yourname]-update-facts-complete</code></p> <p>Now we are ready to move to the cloud function code.</p>"},{"location":"exercises/simple_mlops/step2/#update-the-cloud-function-code","title":"Update the Cloud Function Code","text":"<p>Here are the steps necessary to complete the exercise:</p> <ol> <li> <p>Create the client objects: Use the Google Cloud BigQuery API, and PubSub API to create respective client objects.</p> <pre><code>################\n# 1. Clients ###\n################\nbigquery_client = 'Create a bigquery client here, with the correct project ID argument'\npublisher = 'Create a publisher client here, with the correct project ID argument'\n\nreturn models.GCPClients(\n    bigquery_client=bigquery_client,\n    publisher=publisher\n)\n</code></pre> </li> <li> <p>Set Environment Variables</p> <p>In the <code>b_update_facts/config/dev.env.yaml</code> file, change the environment variables for the correct ones.</p> <pre><code>##############################\n# 2. Environment variables ###\n##############################\n</code></pre> <pre><code>_GCP_PROJECT_ID: \"The GCP project ID where the resources are located\"\n_BIGQUERY_DATASET_ID: \"The BigQuery dataset ID you created\"\n_BIGQUERY_FACTS_TABLE_ID: \"The BigQuery staging table ID\"\n_BIGQUERY_STAGING_TABLE_ID: \"The BigQuery facts table ID where the data will be moved towards\"\n_TOPIC_UPDATE_FACTS_COMPLETE: \"The Pub/Sub topic ID where you will send a message once the data is ingested\"\n</code></pre> </li> <li> <p>Send the correct arguments to the <code>load_query</code> function.</p> <pre><code>#################################################\n# 3. Send the correct arguments to load_query ###\n#################################################\n\npath = Path('./resources/staging_to_facts.sql')\n\nquery = common.load_query(\n    table_facts='??',\n    table_raw='??',\n    query_path=path,\n)\n</code></pre> </li> <li> <p>Send the correct arguments to <code>execute_query_result</code>.</p> <pre><code>#################################################\n# 4. Send the correct arguments execute query ###\n#################################################\n\n_ = gcp_apis.execute_query_result(\n    BQ='??',\n    query='??'\n)\n</code></pre> </li> <li> <p>Publish Message: Correct the arguments in the <code>pubsub_publish_message</code> function, to publish a message.</p> <pre><code>#########################################################\n# 5. Correct the arguments below to publish a message ###\n#########################################################\ngcp_apis.pubsub_publish_message(\n    PS='??',\n    project_id='??',\n    topic_id='??',\n    message=json.dumps({\n        'message': \"I finished passing the staging data to facts\",\n        'training_data_table': '??'}),\n    attributes={\n        'train_model': 'True',\n        'dataset': 'titanic'},\n)\n</code></pre> <p>Hint: you'll have to send to the subscribers which table you added data to.</p> </li> </ol>"},{"location":"exercises/simple_mlops/step2/#deploy-the-cloud-function","title":"Deploy the cloud function","text":"<p>You can check the deployment here in Cloud Build</p> <p>Reference: gcloud functions deploy</p> <pre><code>FUNCTION_NAME=\"update-facts\"\nYOURNAME=\"your_name_in_lowercase\"\n\ngcloud beta functions deploy $YOURNAME-$FUNCTION_NAME \\\n--gen2 --cpu=1 --memory=512MB \\\n--region=europe-west3 \\\n--runtime=python311 \\\n--source=functions/simple_mlops/b_update_facts/app/ \\\n--env-vars-file=functions/simple_mlops/b_update_facts/config/dev.env.yaml \\\n--entry-point=main \\\n--trigger-topic=${YOURNAME}-ingestion-complete\n</code></pre>"},{"location":"exercises/simple_mlops/step2/#documentation","title":"Documentation","text":"<p>Does not work without the code correct.</p>"},{"location":"exercises/simple_mlops/step3/","title":"Deploy a Cloud function that trains a model and saves it in GCS","text":"<ul> <li>Deploy a Cloud function that trains a model and saves it in GCS</li> <li>Introduction</li> <li>Tasks</li> <li>Create the Google Cloud Resources<ul> <li>1. Create the models GCS Bucket</li> </ul> </li> <li>Update the Cloud Function Code</li> <li>Deploy the cloud function</li> <li>Documentation</li> </ul>"},{"location":"exercises/simple_mlops/step3/#introduction","title":"Introduction","text":"<p>In this exercise, we will create a Cloud Function called <code>Train Model</code>, which will be responsible for training a machine learning model using the data ingested in the previous steps. The function will be triggered by the <code>ingestion_complete</code> Pub/Sub topic, ensuring it starts training once new data is available in the BigQuery table. The steps involved in this process are as follows:</p> <ol> <li> <p>The <code>Train Model</code> Cloud Function is subscribed to the <code>[yourname]-update-facts-complete</code> topic, and it will be triggered automatically when a new message is published, indicating that new data has been loaded into the BigQuery table.</p> </li> <li> <p>Upon being triggered, the <code>train_model</code> function retrieves the data from the <code>Titanic Facts</code> BigQuery table using the appropriate query. This data will be used to train a machine learning model, such as a Scikit-learn Random Forest or Logistic Regression model.</p> </li> <li> <p>After the model is trained using the fetched data, the <code>Train Model</code> function saves the trained model to the <code>[yourname]-models</code> Google Cloud Storage bucket. You can choose the name for this model, but it should be unique.</p> </li> </ol> <p>This exercise will guide you through the process of developing the <code>train_model</code> Cloud Function, which leverages the power of BigQuery, Scikit-learn, and Google Cloud Storage to create, train, and store a machine learning model.</p> <p>For this you will need these resources:</p> <ul> <li>The already created Data Set in step 1.</li> <li>The already created Bigquery Table in step 2.</li> <li>A Pub/Sub topic named <code>[yourname]-train_model-complete</code> where you will publish a success message.</li> <li>One GCS Bucket named <code>[yourname]-models</code> where you will save the model</li> </ul> <p>The outline of the Cloud Function code is available at <code>./functions/manual_exercises/c_train_model/app</code></p> <pre><code>c_train_model/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 funcs/\n\u2502   \u2502   \u251c\u2500\u2500 models.py # Models to make typechecking easier.\n\u2502   \u2502   \u251c\u2500\u2500 gcp_apis.py # Functions to call google services.\n\u2502   \u2502   \u251c\u2500\u2500 common.py # Common functions (Utils).\n|   |   \u2514\u2500\u2500 train_model.py # Train model functions\n\u2502   \u251c\u2500\u2500 main.py # Main module and entry point for the Cloud Function\n\u2502   \u2514\u2500\u2500 requirements.txt # Requirements for the function execution.\n\u251c\u2500\u2500 config/\n\u2502   \u2514\u2500\u2500 dev.env.yaml # Environment variables that will ship with the function deployment\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 test_*.py # Unit tests.\n</code></pre>"},{"location":"exercises/simple_mlops/step3/#tasks","title":"Tasks","text":"<ul> <li> Create the Google Cloud Resources</li> <li> Update the Cloud Function Code</li> <li> Test the Cloud Function</li> <li> Deploy the Cloud Function</li> </ul>"},{"location":"exercises/simple_mlops/step3/#create-the-google-cloud-resources","title":"Create the Google Cloud Resources","text":"<p>Here are the resources necessary to complete the exercise:</p> <p>You can create the resources with Cloud Shell or in the Console. The end result will be the same. When creating a resource, choose either to create it with the cloud shell or the console, but not both.</p> <p>For Cloud Shell, set these variables:</p> <pre><code>export PROJECT_NAME=$(gcloud config get-value project)\nexport REGION=europe-west3\nexport YOURNAME=your_name_in_lowercase\n</code></pre> <p></p>"},{"location":"exercises/simple_mlops/step3/#1-create-the-models-gcs-bucket","title":"1. Create the models GCS Bucket","text":"<pre><code>gsutil mb \\\n-c regional \\\n-l ${REGION} \\\n-p ${PROJECT_NAME} \\\ngs://${YOURNAME}-models\n\ngsutil label ch -l owner:${YOURNAME} gs://${YOURNAME}-models\ngsutil label ch -l project:${PROJECT_NAME} gs://${YOURNAME}-models\ngsutil label ch -l purpose:academy gs://${YOURNAME}-models\n</code></pre> <p>Reference: gsutil mb, gsutil label</p> <p>With the console:</p> <p>Same as in step 1, but now the bucket name is <code>[yourname]-models</code></p>"},{"location":"exercises/simple_mlops/step3/#update-the-cloud-function-code","title":"Update the Cloud Function Code","text":"<ol> <li> <p>Create the client objects: Use the Google Cloud BigQuery API, and Storage API to create respective client objects.</p> <pre><code>################\n# 1. Clients ###\n################\nstorage_client = 'Create a storage client here, with the correct project ID argument'\nbigquery_client = 'Create a bigquery client here, with the correct project ID argument'\n\nreturn models.GCPClients(\n    storage_client=storage_client,\n    bigquery_client=bigquery_client,\n)\n</code></pre> </li> <li> <p>Set Environment Variables</p> <p>In the <code>c_train_model/config/dev.env.yaml</code> file, change the environment variables for the correct ones.</p> <pre><code>##############################\n# 2. Environment variables ###\n##############################\n</code></pre> <pre><code>_GCP_PROJECT_ID: \"The GCP project ID where the resources are located\"\n_GCS_BUCKET_NAME_MODELS: \"The GCS bucket name where the models will be saved\"\n_TOPIC_TRAINING_COMPLETE: \"The Pub/Sub topic name where the success message will be published\"\n</code></pre> </li> <li> <p>Create the SQL Query</p> <p>You can find the query to change in the <code>c_train_model/app/resources/select_train_data.sql</code> file.</p> <pre><code>########################################################\n# 3. Create a query that retrieves the training data ###\n########################################################\nquery = common.query_train_data(\n    table_fqn='??'\n    query_path=path\n)\n</code></pre> <pre><code>SELECT 'THIS QUERY IS NOT IMPLEMENTED YET' FROM `{table_source}`\n</code></pre> <p>Do not change to `{table_source}` section.</p> </li> <li> <p>Correct the arguments in the <code>model_save_to_storage</code> function</p> <pre><code>gcp_apis.model_save_to_storage(\n    CS='??',\n    model='??',\n    bucket_name='??'\n)\n</code></pre> </li> </ol>"},{"location":"exercises/simple_mlops/step3/#deploy-the-cloud-function","title":"Deploy the cloud function","text":"<p>You can check the deployment here in Cloud Build</p> <p>Reference: gcloud functions deploy</p> <pre><code>FUNCTION_NAME=\"train_model\"\nYOURNAME=\"your_name_in_lowercase\"\n\ngcloud beta functions deploy $YOURNAME-$FUNCTION_NAME \\\n--gen2 --cpu=1 --memory=512MB \\\n--region=europe-west3 \\\n--runtime=python311 \\\n--source=functions/simple_mlops/c_train_model/app/ \\\n--env-vars-file=functions/simple_mlops/c_train_model/config/dev.env.yaml \\\n--entry-point=main \\\n--trigger-topic=$YOURNAME-update-facts-complete\n</code></pre>"},{"location":"exercises/simple_mlops/step3/#documentation","title":"Documentation","text":"<p>Does not work without the code correct.</p>"},{"location":"exercises/simple_mlops/step4/","title":"Create an endpoint to serve the model to the outside world","text":"<p>In this exercise, you'll be working with the <code>predictions_endpoint</code> Cloud Function. This HTTP-triggered function serves as the prediction endpoint for clients to send new data points. Upon receiving a request containing new data, the function performs the following steps:</p> <ol> <li>It loads the previously trained model from the <code>[yourname]-models</code> bucket into memory.</li> <li>Utilizing the loaded model, it generates a prediction based on a data point received in an HTTP request.</li> <li>The function then stores both the prediction and the new data in the <code>Titanic Prediction</code> BigQuery table to maintain a record of all predictions.</li> <li>Finally, it returns the prediction result to the client, completing the request-response cycle.</li> </ol> <p>Your task is to create the resources necessary and deploy the function.</p> <p>The outline of the Cloud Function code is available at <code>./functions/simple_mlops/d_predictions_endpoint/</code></p>"},{"location":"exercises/simple_mlops/step4/#tasks","title":"Tasks","text":"<ul> <li> Create the <code>Titanic Predictions</code> Table</li> <li>The table is schena is at <code>infrastructure/bigquery/titanic_predictions.json</code></li> <li> Change the configurations in the <code>dev.env.yaml</code> file</li> <li> Change the deployment command to deploy the function correctly.</li> </ul>"},{"location":"exercises/simple_mlops/step4/#deployment","title":"Deployment","text":"<p>Deployment:</p> <pre><code>FUNCTION_NAME=\"predictions_endpoint\"\nYOURNAME=\"your_name_in_lowercase\"\n\ngcloud beta functions deploy $YOURNAME-$FUNCTION_NAME \\\n--gen2 --cpu=1 --memory=1024MB \\\n--region=europe-west3 \\\n--runtime=python311 \\\n--source=functions/simple_mlops/d_predictions_endpoint/app/ \\\n--env-vars-file=functions/simple_mlops/d_predictions_endpoint/config/dev.env.yaml \\\n--allow-unauthenticated \\\n--entry-point=?? \\\n--trigger-????\n</code></pre> <p>And then you can test it on on Stackblitz and change the <code>TitanicEndpoint</code> variable in <code>./src/app/titanic-prediction.service.ts</code>.</p>"},{"location":"exercises/simple_mlops/step4/#documentation","title":"Documentation","text":"<p>Does not work without the code correct.</p>"},{"location":"exercises/simple_mlops/step5/","title":"(Extra) Retraining the model","text":"<p>In this final exercise, you'll focus on extending an existing diagram to include the model retraining cycle. Using a diagramming tool like draw.io or Excalidraw, you will represent how the retraining cycle integrates into the current architecture. The diagram should include the following element:</p> <ul> <li>Incorporate the model retraining cycle into the architecture. Show how the system monitors prediction performance, triggers model retraining when necessary, and updates the stored model in the my-model-storage bucket. You may also want to include a mechanism for evaluating the new model's performance and deciding whether to replace the existing model or keep it.</li> </ul> <p>Your task is to design a clear and informative diagram that visually communicates how the model retraining cycle interacts with the existing architecture, helping others better understand the complete system.</p>"},{"location":"services/","title":"Services","text":"<ul> <li>Services<ul> <li>Google Cloud Storage</li> <li>Cloud Functions</li> <li>BigQuery</li> <li>Pub/Sub (Cloud Pub/Sub)</li> <li>Comparison with other providers</li> </ul> </li> </ul> <p>In the world of cloud computing, various cloud service providers offer a wide range of services to cater to different business needs. In this document, we'll introduce and compare some essential services offered by Google Cloud Platform (GCP), along with their counterparts in Amazon Web Services (AWS) and Microsoft Azure.</p>"},{"location":"services/#google-cloud-storage","title":"Google Cloud Storage","text":"<p>Google Cloud Storage (GCS) is a scalable, fully-managed, and highly available object storage service provided by Google Cloud Platform. It allows users to store, access, and manage data across multiple storage classes, catering to various use cases like backup, archival, and content delivery. GCS ensures data durability and offers seamless integration with other Google Cloud services.</p> <p>Alternative Services:</p> <ul> <li>AWS: Amazon S3 (Simple Storage Service)</li> <li>Azure: Azure Blob Storage</li> </ul>"},{"location":"services/#cloud-functions","title":"Cloud Functions","text":"<p>Cloud Functions is Google Cloud's serverless compute service. It enables you to run event-driven code without managing servers. You can trigger functions in response to various events, such as HTTP requests, changes in Cloud Storage, or Pub/Sub messages.</p> <p>Alternative Services:</p> <ul> <li>AWS: AWS Lambda</li> <li>Azure: Azure Functions</li> </ul>"},{"location":"services/#bigquery","title":"BigQuery","text":"<p>BigQuery is a fully-managed, serverless, petabyte-scale data warehouse by Google Cloud Platform. It enables super-fast SQL queries using the processing power of Google's infrastructure, allowing users to analyze large datasets in real-time. BigQuery is designed for scalability, ease of use, and integration with other Google Cloud services.</p> <p>Alternative Services:</p> <ul> <li>AWS: Amazon Redshift</li> <li>Azure: Azure Synapse Analytics (formerly SQL Data Warehouse)</li> </ul>"},{"location":"services/#pubsub-cloud-pubsub","title":"Pub/Sub (Cloud Pub/Sub)","text":"<p>Pub/Sub stands for \"publish/subscribe\" and is Google Cloud's messaging service. It allows you to build event-driven systems by decoupling the senders and receivers of messages. It can handle real-time data streaming, event notifications, and asynchronous communication.</p> <p>Alternative Services:</p> <ul> <li>AWS: Amazon SNS (Simple Notification Service) and Amazon SQS (Simple Queue Service)</li> <li>Azure: Azure Service Bus and Azure Event Hubs</li> </ul>"},{"location":"services/#comparison-with-other-providers","title":"Comparison with other providers","text":"<p>These cloud services provide the fundamental building blocks for modern cloud-based applications, and each cloud provider offers its own unique features and integrations to meet specific business needs.</p> Service Google Cloud Platform AWS Azure GCS Object storage S3 Blob Storage Cloud Functions Serverless computing Lambda Functions BigQuery Cloud data warehouse Redshift Synapse Analytics Cloud Pub/Sub Messaging SNS/SQS Event Hubs"},{"location":"services/bigquery/","title":"Bigquery","text":"<ul> <li>Bigquery<ul> <li>History</li> <li>Features</li> <li>SDK</li> <li>Useful links</li> </ul> </li> </ul>"},{"location":"services/bigquery/#history","title":"History","text":"<p>BigQuery was announced in 2011 and made generally available in 2012. It was developed by Google engineers who were working on the Google Analytics platform. The goal of BigQuery was to create a cloud-based data warehouse that would be able to handle the massive amounts of data that Google Analytics was generating.</p> <p>BigQuery is a serverless data warehouse, which means that you don't have to worry about managing the underlying infrastructure. You simply upload your data to BigQuery and then run queries against it. BigQuery uses a distributed computing architecture to process queries, which allows it to scale to handle very large datasets.</p> <p>BigQuery is also a cost-effective data warehouse. You only pay for the data that you store and the queries that you run. This makes it a good choice for businesses that need to store and analyze large amounts of data, but don't want to spend a lot of money on a traditional data warehouse.</p> <p>BigQuery has been used by a variety of businesses, including Netflix, Coca-Cola, and the US Department of Defense. It is a popular choice for businesses that need to store and analyze large amounts of data quickly and cost-effectively.</p> <p>Here are some of the key milestones in the history of BigQuery:</p> <ul> <li>2011: BigQuery is announced at the Google Cloud Next conference.</li> <li>2012: BigQuery is made generally available.</li> <li>2013: BigQuery adds support for the BigQuery ML machine learning engine.</li> <li>2014: BigQuery adds support for the BigQuery BI Engine, which allows users to run interactive queries against BigQuery data.</li> <li>2015: BigQuery adds support for the BigQuery Data Transfer Service, which makes it easier to load data into BigQuery from a variety of sources.</li> <li>2016: BigQuery adds support for the BigQuery Data Studio, which is a cloud-based data visualization tool.</li> <li>2017: BigQuery adds support for the BigQuery Data Fusion, which is a fully managed, cloud native ETL service.</li> <li>2018: BigQuery adds support for the BigQuery Federated Data Access, which allows users to query data that is stored in other data warehouses.</li> <li>2019: BigQuery adds support for the BigQuery Omni, which is a fully managed, cloud native data lakehouse.</li> <li>2020: BigQuery adds support for the BigQuery Geospatial Engine, which allows users to run geospatial queries against BigQuery data.</li> <li>2021: BigQuery adds support for the BigQuery ML Engine Pipelines, which makes it easier to build and deploy machine learning pipelines.</li> </ul> <p>BigQuery is a continually evolving platform, and new features are being added all the time. It is a powerful tool for businesses that need to store and analyze large amounts of data.</p>"},{"location":"services/bigquery/#features","title":"Features","text":"<ul> <li>Scalability: BigQuery uses a distributed computing architecture to process queries, which allows it to scale to handle very large datasets. The maximum size of a BigQuery table is 10 petabytes.</li> <li>Cost-effectiveness: You only pay for the data that you store and the queries that you run. There are no upfront costs or commitments.</li> <li>Performance: BigQuery can process queries very quickly, even on large datasets. The average query latency is less than 1 second.</li> <li>Flexibility: BigQuery supports a variety of data formats, including CSV, JSON, and Avro. It can be used to analyze a wide variety of data types, including structured, semi-structured, and unstructured data.</li> <li>Security: BigQuery is a secure platform that meets the needs of businesses of all sizes. It is compliant with a variety of industry standards, including HIPAA and GDPR.</li> <li>Integrations: BigQuery integrates with other Google Cloud Platform services, such as Cloud Dataproc and Cloud Dataflow. This makes it easy to build and deploy data pipelines that can process data from a variety of sources.</li> <li>Machine learning: BigQuery has built-in machine learning capabilities that can be used to analyze data and extract insights. This can be used for tasks such as fraud detection, customer segmentation, and product recommendations.</li> </ul>"},{"location":"services/bigquery/#sdk","title":"SDK","text":"<p>BigQuery has SDKs for many popular programming languages, including:</p> <ul> <li>Python</li> <li>Java</li> <li>Go</li> <li>JavaScript</li> <li>R</li> <li>C++</li> <li>PHP</li> <li>Ruby</li> <li>Swift</li> <li>Kotlin</li> <li>Dart</li> </ul> <p>You can find the documentation for the BigQuery SDK for your preferred programming language on the Google Cloud Platform website.</p>"},{"location":"services/bigquery/#useful-links","title":"Useful links","text":"<ul> <li>BigQuery Documentation</li> <li>Query Syntax</li> <li>BigQuery Data Manipulation Language (DML)</li> <li>BigQuery Data Definition Language (DDL)</li> </ul>"},{"location":"services/cloud_storage/","title":"Cloud Storage","text":"<p>WIP.</p>"},{"location":"services/message_queue/","title":"Publisher-Subscriber and Pub/Sub","text":""},{"location":"services/message_queue/#what-is-the-publisher-subscriber-pattern","title":"What is the Publisher-Subscriber pattern?","text":"<pre><code>The publisher-subscriber (pub-sub) messaging pattern is a communication paradigm where messages are sent by publishers to multiple subscribers, without requiring direct connections between them. Publishers broadcast messages to topics, and subscribers listen to topics they are interested in. This pattern provides a decoupled architecture, allowing for scalability, flexibility, and fault tolerance. Subscribers receive messages asynchronously, enabling them to process events independently, without blocking or waiting for other subscribers. The pub-sub pattern is widely used in distributed systems, event-driven architectures, and messaging applications.\n</code></pre>"},{"location":"services/message_queue/#what-is-google-pubsub","title":"What is Google Pub/Sub?","text":"<pre><code>Google Pub/Sub is a real-time messaging service based on the publisher-subscriber pattern, designed for Google Cloud Platform. It enables reliable, scalable, and asynchronous event-driven communication between microservices, applications, and data streams, promoting decoupled and flexible architectures.\n</code></pre>"},{"location":"services/serverless_computing/","title":"Serverless","text":"<ul> <li>Serverless<ul> <li>History<ul> <li>1. Early Web Hosting and Managed Services</li> <li>2. Rise of PaaS (Platform as a Service)</li> <li>3. Introduction of Amazon AWS Lambda (2014)</li> <li>4. Google Cloud Functions (2017)</li> <li>5. Serverless Frameworks and Ecosystem Growth</li> <li>6. Azure Functions and Other Cloud Providers</li> <li>7. Serverless Adoption and Use Cases</li> <li>8. Evolving Serverless Features</li> </ul> </li> <li>What is Serverless?</li> <li>Cloud Functions SDK</li> </ul> </li> </ul>"},{"location":"services/serverless_computing/#history","title":"History","text":"<p>Serverless computing has evolved over the years and has become a popular paradigm for building scalable and cost-effective cloud applications. Below is a brief history of serverless computing and how it led to the creation of AWS Lambda and Google Cloud Functions:</p>"},{"location":"services/serverless_computing/#1-early-web-hosting-and-managed-services","title":"1. Early Web Hosting and Managed Services","text":"<p>Before serverless computing became a distinct concept, cloud providers offered various forms of managed services and web hosting solutions. These services allowed developers to deploy applications without worrying about server management but still required them to provision and manage servers manually.</p>"},{"location":"services/serverless_computing/#2-rise-of-paas-platform-as-a-service","title":"2. Rise of PaaS (Platform as a Service)","text":"<p>Platform as a Service (PaaS) offerings, such as Google App Engine and Heroku, started to gain popularity. These platforms abstracted away even more of the infrastructure management tasks, allowing developers to focus solely on writing code and deploying applications.</p>"},{"location":"services/serverless_computing/#3-introduction-of-amazon-aws-lambda-2014","title":"3. Introduction of Amazon AWS Lambda (2014)","text":"<p>In November 2014, AWS Lambda was introduced by Amazon Web Services. AWS Lambda is often considered a pioneering service in the serverless computing space. It allows developers to run code in response to events, such as HTTP requests, file uploads, database changes, or scheduled events, without managing servers. AWS Lambda introduced the concept of \"functions\" where developers could write code in response to specific triggers.</p>"},{"location":"services/serverless_computing/#4-google-cloud-functions-2017","title":"4. Google Cloud Functions (2017)","text":"<p>In March 2017, Google Cloud Functions was launched as Google's serverless computing offering. Similar to AWS Lambda, Google Cloud Functions enables developers to run event-driven functions in response to various triggers within the Google Cloud ecosystem. This service is tightly integrated with other Google Cloud services like Cloud Storage, Pub/Sub, and BigQuery.</p>"},{"location":"services/serverless_computing/#5-serverless-frameworks-and-ecosystem-growth","title":"5. Serverless Frameworks and Ecosystem Growth","text":"<p>As serverless computing gained popularity, a vibrant ecosystem of serverless frameworks, tools, and libraries emerged. These frameworks, like the Serverless Framework and AWS SAM (Serverless Application Model), aimed to simplify the development and deployment of serverless applications across multiple cloud providers.</p>"},{"location":"services/serverless_computing/#6-azure-functions-and-other-cloud-providers","title":"6. Azure Functions and Other Cloud Providers","text":"<p>Microsoft Azure introduced Azure Functions, its serverless computing offering, which allows developers to build and run event-driven functions in the Azure cloud environment. Other cloud providers, including IBM Cloud, Oracle Cloud, and Alibaba Cloud, also introduced their own serverless offerings, expanding the availability of serverless computing to a wider audience.</p>"},{"location":"services/serverless_computing/#7-serverless-adoption-and-use-cases","title":"7. Serverless Adoption and Use Cases","text":"<p>Serverless computing has found adoption across various industries and use cases, including web applications, IoT, real-time data processing, and more. It has become a fundamental part of modern cloud application architecture, offering benefits like auto-scaling, cost optimization, and reduced operational overhead.</p>"},{"location":"services/serverless_computing/#8-evolving-serverless-features","title":"8. Evolving Serverless Features","text":"<p>Over time, serverless platforms like AWS Lambda and Google Cloud Functions have continued to evolve, introducing new features, language support, and integrations. This evolution has made it easier for developers to build complex and scalable applications while embracing the serverless paradigm.</p>"},{"location":"services/serverless_computing/#what-is-serverless","title":"What is Serverless?","text":"<p>Serverless computing is a cloud computing model that abstracts away the underlying infrastructure and server management, allowing developers to focus solely on writing and deploying code. In a serverless architecture, developers write functions or small units of code that are executed in response to specific events or triggers, without the need to provision, configure, or manage servers. Here are key characteristics and concepts of serverless computing:</p> <ol> <li> <p>Event-Driven Execution: Serverless functions are typically triggered by events, such as HTTP requests, database changes, file uploads, timers, or messages from message queues. These events initiate the execution of the function, and the function processes the event data.</p> </li> <li> <p>Automatic Scaling: Serverless platforms automatically scale the number of function instances up or down based on the incoming workload. This ensures that functions can handle varying levels of traffic and scale to meet demand without manual intervention.</p> </li> <li> <p>Stateless and Statelessless: Serverless functions are designed to be stateless, meaning they don't maintain any persistent state between invocations. Any required state should be stored externally, such as in a database or storage service.</p> </li> <li> <p>Pay-Per-Use Billing: With serverless computing, you pay only for the compute resources used during the execution of your functions. There are no upfront costs or charges for idle resources, making it cost-effective for applications with variable workloads.</p> </li> <li> <p>Short-Lived Execution: Serverless functions are typically designed to execute quickly, usually completing their tasks in seconds to a few minutes. Long-running processes may be better suited for other computing models.</p> </li> <li> <p>Managed Services: Serverless platforms, offered by cloud providers like AWS Lambda, Google Cloud Functions, and Azure Functions, handle server provisioning, maintenance, and scaling. This offloads the operational burden from developers.</p> </li> <li> <p>Event Sources and Triggers: Events can come from various sources and triggers, such as HTTP requests (API Gateway), database changes (DynamoDB streams, triggers), file uploads (storage services), messages (message queues like AWS SQS or Pub/Sub), or scheduled events (cron-like triggers).</p> </li> <li> <p>Language Support: Serverless platforms typically support multiple programming languages, allowing developers to write functions in their preferred language.</p> </li> <li> <p>Ephemeral Compute: Serverless functions are ephemeral, meaning they are created, executed, and then destroyed. This differs from traditional server-based computing, where servers may persist for longer durations.</p> </li> </ol> <p>Serverless computing is well-suited for a wide range of use cases, including web applications, microservices, real-time data processing, IoT applications, and more. It offers benefits like cost savings, scalability, reduced operational complexity, and faster development cycles. However, it's important to consider the statelessness and execution duration limitations when designing serverless applications, as these factors can impact application architecture and design decisions.</p>"},{"location":"services/serverless_computing/#cloud-functions-sdk","title":"Cloud Functions SDK","text":"<p>Developing Google Cloud Functions (GCF) typically involves writing code in one of the supported programming languages and using the respective Google Cloud SDKs (Software Development Kits) and libraries for that language. The sdk used is the Google Cloud Functions Framework.</p> <ol> <li> <p>Python:</p> <ul> <li>Google Cloud Functions Framework for Python: This is the official library for developing Python-based Google Cloud Functions. It offers a Flask-like interface for building HTTP-triggered functions.</li> </ul> </li> <li> <p>Node.js (JavaScript/TypeScript):</p> <ul> <li>Google Cloud Functions Framework for Node.js: This is an official Google library that simplifies the development of Node.js Cloud Functions. It allows you to write functions as Express.js or HTTP functions.</li> <li><code>@google-cloud/functions-framework</code>: This is another library provided by Google that enables you to run Node.js functions locally for testing.</li> </ul> </li> <li> <p>Go:</p> <ul> <li><code>github.com/GoogleCloudPlatform/functions-framework-go</code>: This community-supported library lets you build Go-based Google Cloud Functions using the Functions Framework.</li> </ul> </li> <li> <p>Java:</p> <ul> <li>Google Cloud Functions Framework for Java: This is the official library for Java-based Google Cloud Functions. It provides an easy way to write and deploy Java functions.</li> </ul> </li> <li> <p>.NET (C#):</p> <ul> <li>Google.Cloud.Functions.Framework: This is a community-supported library that helps you build Google Cloud Functions using .NET Core and C#.</li> </ul> </li> </ol>"}]}